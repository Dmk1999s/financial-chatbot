import uuid
from openai import OpenAI
from dotenv import load_dotenv
import os
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from django.core.cache import cache
from django.contrib.auth.models import User
import re
import ast
import json
from functools import partial, lru_cache

load_dotenv()

# Optimized OpenAI client with caching
class OptimizedOpenAIClient:
    def __init__(self):
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            max_retries=2,
            timeout=30.0
        )
    
    def create_completion(self, messages, model="gpt-3.5-turbo", **kwargs):
        # Check cache first for repeated queries
        prompt_str = str(messages)
        cache_key = f"gpt_response_{hash(prompt_str)}"
        cached = cache.get(cache_key)
        if cached:
            return cached
            
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs
        )
        
        # Cache for 5 minutes
        cache.set(cache_key, response, 300)
        return response

client = OptimizedOpenAIClient()
fine_tuned_model = "ft:gpt-3.5-turbo-0125:personal::BDpYRjbn"
store = {}
SESSION_TEMP_STORE = {}
REQUIRED_KEYS = {
    "age", "risk_tolerance", "income_stability", "income_sources",
    "income", "period", "expected_income", "expected_loss",
    "purpose", "value_growth",
    "risk_acceptance_level", "investment_concern"
}

finetune_prompt = f"""
1. ë„ˆëŠ” ê¸ˆìœµìƒí’ˆ ì¶”ì²œ ì–´í”Œì— íƒ‘ì¬ëœ ì±—ë´‡ì´ë©°, ì´ë¦„ì€ 'ì±—ë´‡'ì´ë‹¤.
2. í•œêµ­ì–´ë¡œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
3. ì‚¬ìš©ìì—ê²Œ ë‹¤ìŒ í•­ëª©ì„ ìˆœì„œëŒ€ë¡œ ë¬¼ì–´ë´ì•¼ í•œë‹¤:
- age: ë‚˜ì´ (ì •ìˆ˜)
- risk_tolerance: ìœ„í—˜ í—ˆìš© ì •ë„ (ì˜ˆ: ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ)
- income: ì—°ì†Œë“ (ì •ìˆ˜, ë‹¨ìœ„ëŠ” ì›)
- income_stability: ì†Œë“ ì•ˆì •ì„± (ì˜ˆ: ì•ˆì •ì , ë¶ˆì•ˆì •)
- income_sources: ì†Œë“ì› (ì˜ˆ: ì•„ë¥´ë°”ì´íŠ¸, ì›”ê¸‰ ë“±)
- period: íˆ¬ì ê¸°ê°„ (ì˜ˆ: ì •ìˆ˜, ë‹¨ìœ„ëŠ” ì¼)
- expected_income: ê¸°ëŒ€ ìˆ˜ìµ (ì •ìˆ˜, ë‹¨ìœ„ëŠ” ì›)
- expected_loss: ì˜ˆìƒ ì†ì‹¤ (ì •ìˆ˜, ë‹¨ìœ„ëŠ” ì›)
- purpose: íˆ¬ì ëª©ì  (ì˜ˆ: ì•ˆì •ì ì¸ ì£¼ì‹ ì¶”ì²œ)
- asset_allocation_type: ìì‚° ë°°ë¶„ ìœ í˜• (0~4ì˜ ì •ìˆ˜. 0: 10% ë¯¸ë§Œ, 1: 10~20%, 2: 20~30%, 3: 30~40%, 4: 40% ì´ìƒ)
- value_growth: ê°€ì¹˜ ë˜ëŠ” ì„±ì¥ (0~1ì˜ ì •ìˆ˜. 0: ê°€ì¹˜, 1: ì„±ì¥)
- risk_acceptance_level: ìœ„í—˜ ìˆ˜ìš© ìˆ˜ì¤€ (1~4ì˜ ì •ìˆ˜. 1: ë¬´ì¡°ê±´ íˆ¬ìì›ê¸ˆ ë³´ì¡´, 2: ì´ììœ¨ ìˆ˜ì¤€ì˜ ìˆ˜ìµ ë° ì†ì‹¤ ê¸°ëŒ€, 3: ì‹œì¥ì— ë¹„ë¡€í•œ ìˆ˜ìµ ë° ì†ì‹¤ ê¸°ëŒ€, 4: ì‹œì¥ìˆ˜ìµë¥  ì´ˆê³¼ ìˆ˜ìµ ë° ì†ì‹¤ ê¸°ëŒ€) 
- investment_concern: íˆ¬ì ê´€ë ¨ ê³ ë¯¼ (ì˜ˆ: ì–´ë–¤ ì£¼ì‹ì„ ì‚´ì§€ ëª¨ë¦„)

4. ê° í•­ëª©ì„ ì‚¬ìš©ìê°€ ëª¨ë‘ ì‘ë‹µí•˜ë©´ "ì´ì œ ê¸ˆìœµìƒí’ˆì„ ì¶”ì²œí•´ì¤„ê²Œìš”!" ë¼ëŠ” ë§ì„ í•˜ë©° ëŒ€í™”ë¥¼ ëë‚¸ë‹¤.
"""

gpt_prompt = """
ë„ˆëŠ” ì˜¤ì§ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ëŠ” íŒŒì„œì•¼.
ì ˆëŒ€ ì§ˆë¬¸ì´ë‚˜ ëŒ€ë‹µ ì—†ì´ JSONë§Œ ì‘ë‹µí•´. ì´ ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” í—ˆìš©ë˜ì§€ ì•Šì•„.
ë‹¤ìŒì€ JSON ì˜ˆì‹œì•¼:
{
  "age": 25,
  "income": 4000000,
  "income_sources": "ì•„ë¥´ë°”ì´íŠ¸",
  "income_stability": "ë¶ˆì•ˆì •",
  "period": 30,
  "expected_income": 300000,
  "expected_loss": 100000,
  "purpose": "ë‹¨ê¸° ìˆ˜ìµ",
  "asset_allocation_type": 2,
  "value_growth": 1,
  "risk_acceptance_level": 3,
  "investment_concern": "ë¬´ìŠ¨ ì£¼ì‹ì„ ì‚¬ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´ìš”",
  "risk_tolerance": "ì¤‘ê°„"
}
ëª¨ë“  í•­ëª©ì´ ì—†ì„ ê²½ìš°ì—ëŠ” ë°˜ë“œì‹œ ë¹ˆ ê°ì²´ë§Œ ì¶œë ¥í•´: {}
JSON ì™¸ì˜ ë¬¸ì¥ì´ í•œ ì¤„ì´ë¼ë„ ìˆìœ¼ë©´ ì˜¤ë¥˜ì•¼. ë°˜ë“œì‹œ ì§€ì¼œ.
"""

@lru_cache(maxsize=1000)
def get_cached_session_id():
    return str(uuid.uuid4())

def get_session_id(request_data):
    session_id = request_data.get("session_id")
    if not session_id:
        # Use a deterministic session ID for first-time users to enable caching
        username = request_data.get("username", "anonymous")
        session_id = f"new_{username}_{hash(str(request_data)) % 10000}"
    return session_id

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def convert_history_to_openai_format(history):
    role_map = {"human": "user", "ai": "assistant", "system": "system"}
    return [{"role": role_map.get(msg.type, msg.type), "content": msg.content} for msg in history]

def check_conflict(current_data, new_fields):
    conflicting_fields = []
    for key, new_value in new_fields.items():
        if key in current_data and current_data[key] != new_value:
            conflicting_fields.append((key, current_data[key], new_value))
    return conflicting_fields




def extract_json_from_response(text: str):
    try:
        # ë°±í‹± ë¸”ëŸ­ ì œê±° (```json ~ ```)
        cleaned_text = re.sub(r"```json|```", "", text).strip()

        # ì¤‘ê´„í˜¸ ê°ì‹¸ì§„ JSON í…ìŠ¤íŠ¸ ì¶”ì¶œ
        match = re.search(r"\{.*\}", cleaned_text, re.DOTALL)
        if match:
            return json.loads(match.group())
        else:
            print("â— JSON í˜•ì‹ì´ ì•„ë‹˜. ì‘ë‹µ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            return {}
    except Exception:
        # íŒŒì‹± ì¤‘ ì—ëŸ¬ë‚˜ë©´ ë¹ˆ dict ë°˜í™˜
        return {}


def extract_fields_from_natural_response(response_text: str, session_id: str) -> dict:
    # Skip extraction for new sessions to avoid slow GPT-4 calls
    if session_id not in SESSION_TEMP_STORE:
        return {}
    
    # Use faster regex patterns for common cases
    fields = {}
    text_lower = response_text.lower()
    
    # Age extraction
    age_match = re.search(r'(\d+)ì‚´|ë‚˜ì´.*?(\d+)|age.*?(\d+)', text_lower)
    if age_match:
        fields['age'] = int(age_match.group(1) or age_match.group(2) or age_match.group(3))
    
    # Income extraction
    income_match = re.search(r'(\d+)ë§Œì›|ì›”ê¸‰.*?(\d+)|ìˆ˜ì….*?(\d+)', text_lower)
    if income_match:
        fields['monthly_income'] = int(income_match.group(1) or income_match.group(2) or income_match.group(3)) * 10000
    
    # Risk tolerance
    if any(word in text_lower for word in ['ì•ˆì „', 'ë³´ìˆ˜ì ', 'ë‚®ìŒ']):
        fields['risk_tolerance'] = 'ë‚®ìŒ'
    elif any(word in text_lower for word in ['ì ê·¹ì ', 'ë†’ìŒ', 'ê³µê²©ì ']):
        fields['risk_tolerance'] = 'ë†’ìŒ'
    elif 'ì¤‘ê°„' in text_lower:
        fields['risk_tolerance'] = 'ì¤‘ê°„'
    
    return fields

def run_gpt(input_data, config, ai_model):
    user_input = input_data["input"]
    session_id = config.get("configurable", {}).get("session_id")
    user_id = config.get("configurable", {}).get("user_id")

    if user_input.strip() in ["ë„¤", "ì•„ë‹ˆì˜¤"] and "conflict_pending" in SESSION_TEMP_STORE:
        pending = SESSION_TEMP_STORE.pop("conflict_pending")
        if user_input.strip() == "ë„¤":
            # ì¶©ëŒ í•­ëª©ì„ ì €ì¥
            SESSION_TEMP_STORE[session_id].update(pending)
            return {"output": "í”„ë¡œí•„ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í• ê²Œìš”."}
        else:
            return {"output": "ê¸°ì¡´ í”„ë¡œí•„ ì •ë³´ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ê³„ì† ì§„í–‰í• ê²Œìš”."}

    current_data = SESSION_TEMP_STORE.get(session_id, {})
    new_fields = extract_fields_from_natural_response(user_input, session_id)

    # í”„ë¡œí•„ ì¶©ëŒ í™•ì¸
    conflicts = check_conflict(current_data, new_fields)
    if conflicts:
        # ì‚¬ìš©ìì—ê²Œ ì—…ë°ì´íŠ¸ ì—¬ë¶€ ì§ˆë¬¸ ìœ ë„
        conflict_messages = "\n".join(
            f"- {k}: ê¸°ì¡´ '{old}' vs ì…ë ¥ '{new}'"
            for k, old, new in conflicts
        )
        clarification = (
            f"ì…ë ¥í•˜ì‹  ì •ë³´ê°€ ê¸°ì¡´ í”„ë¡œí•„ê³¼ ë‹¤ë¦…ë‹ˆë‹¤:\n{conflict_messages}\n"
            "í”„ë¡œí•„ì„ ì—…ë°ì´íŠ¸í• ê¹Œìš”? 'ë„¤' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ë‹µí•´ì£¼ì„¸ìš”."
        )
        SESSION_TEMP_STORE["conflict_pending"] = {
            k: new for k, _, new in conflicts
        }
        return {"output": clarification}

    # í˜„ì¬ ëˆ„ë½ëœ í•„ë“œ ì¶”ì 
    missing_keys = [key for key in REQUIRED_KEYS if key not in current_data or current_data[key] is None]

    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ + ëˆ„ë½ëœ í‚¤ì— ëŒ€í•œ ì§ˆë¬¸ ìœ ë„
    base_prompt = input_data.get("system_prompt", finetune_prompt)
    prompt_addition = ""
    if missing_keys:
        prompt_addition = (
            "ì•„ì§ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
            f"{', '.join(missing_keys)}\n"
            "ì´ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ í†µí•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•œ ë²ˆì— í•˜ë‚˜ì”© í•˜ì„¸ìš”."
        )

    full_prompt = base_prompt + prompt_addition

    history = get_session_history(session_id).messages
    formatted_history = convert_history_to_openai_format(history)

    messages = [{"role": "system", "content": full_prompt}] + formatted_history + [
        {"role": "user", "content": user_input}
    ]

    response = client.create_completion(
        messages=messages,
        model=ai_model,
        temperature=0.0,
        max_tokens=150  # Limit response length for faster processing
    )
    return {"output": response.choices[0].message.content}


def call_gpt_model(prompt: str, session_id: str) -> str:
    input_data = {
        "input": prompt,
        "system_prompt": gpt_prompt
    }

    config = {
        "configurable": {
            "session_id": session_id
        }
    }

    return _run_gpt_parser(input_data, config, "gpt-3.5-turbo")["output"]

def _run_gpt_parser(input_data, config, model):
    messages = [
        {"role": "system", "content": input_data["system_prompt"]},
        {"role": "user", "content": input_data["input"]}
    ]
    response = client.create_completion(
        messages=messages,
        model=model,
        temperature=0.0,
    )
    return {"output": response.choices[0].message.content}

run_gpt_with_model = partial(run_gpt, ai_model=fine_tuned_model)
# Runnable êµ¬ì„±
runnable = RunnableLambda(run_gpt_with_model)

with_message_history = RunnableWithMessageHistory(
    runnable,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

"""
ì •ë³´ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
"""
def save_profile_from_gpt(parsed_data, user_id, session_id):
    print("DEBUG: save the data")
    try:
        user = User.objects.get(email=user_id)  # ê¸°ì¡´ ìœ ì € ì¡°íšŒ
        user.age = parsed_data.get("age")
        user.income_stability = parsed_data.get("income_stability")
        user.expected_loss = parsed_data.get("expected_loss")
        session_id=session_id,
        user.risk_tolerance=parsed_data.get("risk_tolerance")
        user.income_source=parsed_data.get("income_sources")
        user.income=parsed_data.get("income")
        user.period=parsed_data.get("period")
        user.expected_income=parsed_data.get("expected_income")
        user.expected_loss=parsed_data.get("expected_loss")
        user.purpose=parsed_data.get("purpose")
        user.save()
        print(f"ğŸ” ì €ì¥ëœ user: {user.__dict__}")
    except Exception as e:
        print(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")


"""
views.pyì— ì œê³µí•˜ëŠ” í•¨ìˆ˜
"""
def handle_chat(user_input, session_id, user_id=None):
    # Fast path for new sessions - use lighter processing
    if session_id.startswith("new_"):
        # Initialize session store
        if session_id not in SESSION_TEMP_STORE:
            SESSION_TEMP_STORE[session_id] = {}
        
        # Use simplified prompt for first interaction
        simple_prompt = "ì•ˆë…•í•˜ì„¸ìš”! íˆ¬ì ìƒë‹´ì„ ë„ì™€ë“œë¦´ê²Œìš”. ë¨¼ì € ë‚˜ì´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        
        # Extract basic info from user input
        extracted_fields = extract_fields_from_natural_response(user_input, session_id)
        valid_fields = {
            k: v for k, v in extracted_fields.items()
            if k in REQUIRED_KEYS and v is not None
        }
        SESSION_TEMP_STORE[session_id].update(valid_fields)
        
        return simple_prompt, session_id
    
    # Regular processing for existing sessions
    result = with_message_history.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": session_id}}
    )

    gpt_reply = result["output"]
    extracted_fields = extract_fields_from_natural_response(gpt_reply, session_id)

    if session_id not in SESSION_TEMP_STORE:
        SESSION_TEMP_STORE[session_id] = {}

    valid_fields = {
        k: v for k, v in extracted_fields.items()
        if k in REQUIRED_KEYS and v is not None
    }
    SESSION_TEMP_STORE[session_id].update(valid_fields)

    current_data = SESSION_TEMP_STORE[session_id]

    if REQUIRED_KEYS.issubset(current_data.keys()):
        if user_id:
            save_profile_from_gpt(current_data, user_id, session_id)
        del SESSION_TEMP_STORE[session_id]

    return gpt_reply, session_id
